{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Gen AI APP Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1bed5406f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.langchain.com/langsmith/evaluation-quickstart\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='Evaluation quickstart - Docs by LangChainOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationQuickstartsEvaluation quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageLangSmith SDK1. Install dependencies2. Create a LangSmith API key3. Set up environment variables4. Create a dataset5. Define what you’re evaluating6. Define evaluator7. Run and view resultsNext stepsLangSmith UI1. Navigate to the playground2. Create a prompt3. Create a dataset4. Add an evaluator5. Run your evaluationNext stepsVideo guideQuickstartsEvaluation quickstartCopy pageCopy pageEvaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don’t always behave predictably — small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.\\nEvaluations are made up of three components:\\n\\nA dataset with test inputs and optionally expected outputs.\\nA target function that defines what you’re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.\\nEvaluators that score your target function’s outputs.\\n\\nThis quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nThis guide uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you’re new to evaluations. If you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.\\n\\u200bLangSmith SDK\\n\\u200b1. Install dependencies\\nPythonTypeScriptCopypip install -U langsmith openevals openai\\n\\nIf you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general - you may define evaluators using arbitrary custom code.\\n\\u200b2. Create a LangSmith API key\\nTo create an API key, head to the Settings page. Then click + API Key.\\n\\u200b3. Set up environment variables\\nThis guide uses OpenAI, but you can adapt it to use any LLM provider.\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\nCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langchain-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n\\n\\u200b4. Create a dataset\\nNext, define example input and reference output pairs that you’ll use to evaluate your app:\\nPythonTypeScriptCopyfrom langsmith import Client\\nclient = Client()\\n\\n# Programmatically create a dataset in LangSmith\\n# For other dataset creation methods, see:\\n# https://docs.smith.langchain.com/langsmith/manage-datasets-programmatically\\n# https://docs.smith.langchain.com/langsmith/manage-datasets-in-application\\ndataset = client.create_dataset(\\n    dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\"\\n)\\n\\n# Create examples\\nexamples = [\\n    {\\n        \"inputs\": {\"question\": \"Which country is Mount Kilimanjaro located in?\"},\\n        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},\\n    },\\n    {\\n        \"inputs\": {\"question\": \"What is Earth\\'s lowest point?\"},\\n        \"outputs\": {\"answer\": \"Earth\\'s lowest point is The Dead Sea.\"},\\n    },\\n]\\n\\n# Add examples to the dataset\\nclient.create_examples(dataset_id=dataset.id, examples=examples)\\n\\n\\u200b5. Define what you’re evaluating\\nNow, define a target function that contains what you’re evaluating. In this guide, we’ll define a target function that contains a single LLM call to answer a question.\\nPythonTypeScriptCopyfrom langsmith import wrappers\\nfrom openai import OpenAI\\n\\n# Wrap the OpenAI client for LangSmith tracing\\nopenai_client = wrappers.wrap_openai(OpenAI())\\n\\n# Define the application logic you want to evaluate inside a target function\\n# The SDK will automatically send the inputs from the dataset to your target function\\ndef target(inputs: dict) -> dict:\\n    response = openai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},\\n            {\"role\": \"user\", \"content\": inputs[\"question\"]},\\n        ],\\n    )\\n    return { \"answer\": response.choices[0].message.content.strip() }\\n\\n\\u200b6. Define evaluator\\nImport a prebuilt prompt from openevals and create an evaluator. outputs are the result of your target function. reference_outputs / referenceOutputs are from the example pairs you defined in step 4 above.\\nCORRECTNESS_PROMPT is just an f-string with variables for \"inputs\", \"outputs\", and \"reference_outputs\". See here for more information on customizing OpenEvals prompts.\\nPythonTypeScriptCopyfrom openevals.llm import create_llm_as_judge\\nfrom openevals.prompts import CORRECTNESS_PROMPT\\n\\ndef correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\\n    evaluator = create_llm_as_judge(\\n        prompt=CORRECTNESS_PROMPT,\\n        model=\"openai:o3-mini\",\\n        feedback_key=\"correctness\",\\n    )\\n    eval_result = evaluator(\\n        inputs=inputs,\\n        outputs=outputs,\\n        reference_outputs=reference_outputs\\n    )\\n    return eval_result\\n\\n\\u200b7. Run and view results\\nFinally, run the experiment!\\nPythonTypeScriptCopy# After running the evaluation, a link will be provided to view the results in langsmith\\nexperiment_results = client.evaluate(\\n    target,\\n    data=\"Sample dataset\",\\n    evaluators=[\\n        correctness_evaluator,\\n        # can add multiple evaluators here\\n    ],\\n    experiment_prefix=\"first-eval-in-langsmith\",\\n    max_concurrency=2,\\n)\\n\\nClick the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of the experiment.\\n\\n\\u200bNext steps\\nTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.\\n\\nFor more details on evaluations, refer to the Evaluation documentation.\\nCheck out the OpenEvals README to see all available prebuilt evaluators and how to customize them.\\nLearn how to define custom evaluators that contain arbitrary code.\\nFor comprehensive descriptions of every class and function see the Python or Typescript SDK references.\\n\\nOr, if you prefer video tutorials, check out the Datasets, Evaluators, and Experiments videos from the Introduction to LangSmith Course.\\n\\u200bLangSmith UI\\n\\u200b1. Navigate to the playground\\nLangSmith’s prompt playground makes it possible to run evaluations over different prompts, new models or test different model configurations. Go to LangSmith’s Playground in the UI.\\n\\u200b2. Create a prompt\\nModify the system prompt to:\\nCopyAnswer the following question accurately:\\n\\n\\u200b3. Create a dataset\\nClick Set up Evaluation, then use the + New button in the dropdown to create a new dataset.\\nAdd the following examples to the dataset:\\nInputsReference Outputsquestion: Which country is Mount Kilimanjaro located in?output: Mount Kilimanjaro is located in Tanzania.question: What is Earth’s lowest point?output: Earth’s lowest point is The Dead Sea.\\nPress Save to save your newly created dataset.\\n\\u200b4. Add an evaluator\\nClick +Evaluator. Select Correctness from the pre-built evaluator options. Press Save.\\n\\u200b5. Run your evaluation\\nPress Start on the top right to run your evaluation. Running this evaluation will create an experiment that you can view in full by clicking the experiment name.\\n\\n\\u200bNext steps\\nTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.\\n\\n\\nFor more details on evaluations, refer to the Evaluation documentation.\\n\\n\\nLearn how to create and manage datasets in the UI\\n\\n\\nLearn how to run an evaluation from the prompt playground\\n\\n\\nIf you prefer video tutorials, check out the Playground videos from the Introduction to LangSmith Course.\\n\\u200bVideo guide\\nWas this page helpful?YesNoTrace an applicationTest promptsAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='Evaluation quickstart - Docs by LangChainOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationQuickstartsEvaluation quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageLangSmith SDK1. Install dependencies2. Create a LangSmith API key3. Set up environment variables4. Create a dataset5. Define what you’re evaluating6. Define evaluator7. Run and view resultsNext stepsLangSmith UI1. Navigate to the playground2. Create a prompt3. Create a'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='up environment variables4. Create a dataset5. Define what you’re evaluating6. Define evaluator7. Run and view resultsNext stepsLangSmith UI1. Navigate to the playground2. Create a prompt3. Create a dataset4. Add an evaluator5. Run your evaluationNext stepsVideo guideQuickstartsEvaluation quickstartCopy pageCopy pageEvaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don’t always behave predictably — small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='Evaluations are made up of three components:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='A dataset with test inputs and optionally expected outputs.\\nA target function that defines what you’re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.\\nEvaluators that score your target function’s outputs.\\n\\nThis quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nThis guide uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you’re new to evaluations. If you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.\\n\\u200bLangSmith SDK\\n\\u200b1. Install dependencies\\nPythonTypeScriptCopypip install -U langsmith openevals openai'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='If you are using yarn as your package manager, you will also need to manually install @langchain/core as a peer dependency of openevals. This is not required for LangSmith evals in general - you may define evaluators using arbitrary custom code.\\n\\u200b2. Create a LangSmith API key\\nTo create an API key, head to the Settings page. Then click + API Key.\\n\\u200b3. Set up environment variables\\nThis guide uses OpenAI, but you can adapt it to use any LLM provider.\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\nCopyexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langchain-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n\\n\\u200b4. Create a dataset\\nNext, define example input and reference output pairs that you’ll use to evaluate your app:\\nPythonTypeScriptCopyfrom langsmith import Client\\nclient = Client()'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b4. Create a dataset\\nNext, define example input and reference output pairs that you’ll use to evaluate your app:\\nPythonTypeScriptCopyfrom langsmith import Client\\nclient = Client()\\n\\n# Programmatically create a dataset in LangSmith\\n# For other dataset creation methods, see:\\n# https://docs.smith.langchain.com/langsmith/manage-datasets-programmatically\\n# https://docs.smith.langchain.com/langsmith/manage-datasets-in-application\\ndataset = client.create_dataset(\\n    dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\"\\n)\\n\\n# Create examples\\nexamples = [\\n    {\\n        \"inputs\": {\"question\": \"Which country is Mount Kilimanjaro located in?\"},\\n        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},\\n    },\\n    {\\n        \"inputs\": {\"question\": \"What is Earth\\'s lowest point?\"},\\n        \"outputs\": {\"answer\": \"Earth\\'s lowest point is The Dead Sea.\"},\\n    },\\n]\\n\\n# Add examples to the dataset\\nclient.create_examples(dataset_id=dataset.id, examples=examples)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='# Add examples to the dataset\\nclient.create_examples(dataset_id=dataset.id, examples=examples)\\n\\n\\u200b5. Define what you’re evaluating\\nNow, define a target function that contains what you’re evaluating. In this guide, we’ll define a target function that contains a single LLM call to answer a question.\\nPythonTypeScriptCopyfrom langsmith import wrappers\\nfrom openai import OpenAI\\n\\n# Wrap the OpenAI client for LangSmith tracing\\nopenai_client = wrappers.wrap_openai(OpenAI())'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='# Wrap the OpenAI client for LangSmith tracing\\nopenai_client = wrappers.wrap_openai(OpenAI())\\n\\n# Define the application logic you want to evaluate inside a target function\\n# The SDK will automatically send the inputs from the dataset to your target function\\ndef target(inputs: dict) -> dict:\\n    response = openai_client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},\\n            {\"role\": \"user\", \"content\": inputs[\"question\"]},\\n        ],\\n    )\\n    return { \"answer\": response.choices[0].message.content.strip() }'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b6. Define evaluator\\nImport a prebuilt prompt from openevals and create an evaluator. outputs are the result of your target function. reference_outputs / referenceOutputs are from the example pairs you defined in step 4 above.\\nCORRECTNESS_PROMPT is just an f-string with variables for \"inputs\", \"outputs\", and \"reference_outputs\". See here for more information on customizing OpenEvals prompts.\\nPythonTypeScriptCopyfrom openevals.llm import create_llm_as_judge\\nfrom openevals.prompts import CORRECTNESS_PROMPT\\n\\ndef correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\\n    evaluator = create_llm_as_judge(\\n        prompt=CORRECTNESS_PROMPT,\\n        model=\"openai:o3-mini\",\\n        feedback_key=\"correctness\",\\n    )\\n    eval_result = evaluator(\\n        inputs=inputs,\\n        outputs=outputs,\\n        reference_outputs=reference_outputs\\n    )\\n    return eval_result'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b7. Run and view results\\nFinally, run the experiment!\\nPythonTypeScriptCopy# After running the evaluation, a link will be provided to view the results in langsmith\\nexperiment_results = client.evaluate(\\n    target,\\n    data=\"Sample dataset\",\\n    evaluators=[\\n        correctness_evaluator,\\n        # can add multiple evaluators here\\n    ],\\n    experiment_prefix=\"first-eval-in-langsmith\",\\n    max_concurrency=2,\\n)\\n\\nClick the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of the experiment.\\n\\n\\u200bNext steps\\nTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.\\n\\nFor more details on evaluations, refer to the Evaluation documentation.\\nCheck out the OpenEvals README to see all available prebuilt evaluators and how to customize them.\\nLearn how to define custom evaluators that contain arbitrary code.\\nFor comprehensive descriptions of every class and function see the Python or Typescript SDK references.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='Or, if you prefer video tutorials, check out the Datasets, Evaluators, and Experiments videos from the Introduction to LangSmith Course.\\n\\u200bLangSmith UI\\n\\u200b1. Navigate to the playground\\nLangSmith’s prompt playground makes it possible to run evaluations over different prompts, new models or test different model configurations. Go to LangSmith’s Playground in the UI.\\n\\u200b2. Create a prompt\\nModify the system prompt to:\\nCopyAnswer the following question accurately:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b3. Create a dataset\\nClick Set up Evaluation, then use the + New button in the dropdown to create a new dataset.\\nAdd the following examples to the dataset:\\nInputsReference Outputsquestion: Which country is Mount Kilimanjaro located in?output: Mount Kilimanjaro is located in Tanzania.question: What is Earth’s lowest point?output: Earth’s lowest point is The Dead Sea.\\nPress Save to save your newly created dataset.\\n\\u200b4. Add an evaluator\\nClick +Evaluator. Select Correctness from the pre-built evaluator options. Press Save.\\n\\u200b5. Run your evaluation\\nPress Start on the top right to run your evaluation. Running this evaluation will create an experiment that you can view in full by clicking the experiment name.\\n\\n\\u200bNext steps\\nTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.\\n\\n\\nFor more details on evaluations, refer to the Evaluation documentation.\\n\\n\\nLearn how to create and manage datasets in the UI'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='For more details on evaluations, refer to the Evaluation documentation.\\n\\n\\nLearn how to create and manage datasets in the UI\\n\\n\\nLearn how to run an evaluation from the prompt playground\\n\\n\\nIf you prefer video tutorials, check out the Playground videos from the Introduction to LangSmith Course.\\n\\u200bVideo guide\\nWas this page helpful?YesNoTrace an applicationTest promptsAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DeLL\\Documents\\GitHub\\OpenAI-and-Ollama\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1bee6fcb8d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A dataset with test inputs and optionally expected outputs.\\nA target function that defines what you’re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.\\nEvaluators that score your target function’s outputs.\\n\\nThis quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.\\nThis guide uses prebuilt LLM-as-judge evaluators from the open-source openevals package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you’re new to evaluations. If you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators using your own code.\\n\\u200bLangSmith SDK\\n\\u200b1. Install dependencies\\nPythonTypeScriptCopypip install -U langsmith openevals openai'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query=\"A target function that defines what you’re evaluating.\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001BEE7CD0ED0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001BEE7CD1BD0>, root_client=<openai.OpenAI object at 0x000001BEE7CD0A50>, root_async_client=<openai.AsyncOpenAI object at 0x000001BEE7CD18D0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evaluations consist of a dataset with test inputs (and optionally expected outputs), a target function that specifies what is being evaluated (such as an LLM call with a new prompt, a part of an application, or an end-to-end application), and evaluators that score the outputs of the target function.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"Evaluations are made up of three components:\",\n",
    "    \"context\":[Document(page_content=\"Evaluations are made up of three components:A dataset with test inputs and optionally expected outputs.A target function that defines what you’re evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.Evaluators that score your target functions outputs.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1bee6fcb8d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001BEE6FCB8D0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001BEE7CD0ED0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001BEE7CD1BD0>, root_client=<openai.OpenAI object at 0x000001BEE7CD0A50>, root_async_client=<openai.AsyncOpenAI object at 0x000001BEE7CD18D0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, one way to evaluate the performance of LLM applications is through the use of evaluations, which are quantitative methods designed to measure how well these models perform. This involves setting up environment variables, creating datasets, defining what you are evaluating, adding evaluators, and running the evaluations to view results. For instance, in the context provided, a dataset is created with specific input-output pairs (like questions and their correct answers), and an evaluator is added to assess correctness. The evaluation is then run to produce an experiment that analyzes the results, helping in identifying any failures and aiding in the development of more reliable AI applications.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"Evaluations are made up of three components:\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Evaluations are made up of three components:',\n",
       " 'context': [Document(id='7f5c009f-d7ee-47dd-a31f-61ba18708222', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='Evaluations are made up of three components:'),\n",
       "  Document(id='de2e94eb-261c-4915-84d5-7e539df4e286', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='up environment variables4. Create a dataset5. Define what you’re evaluating6. Define evaluator7. Run and view resultsNext stepsLangSmith UI1. Navigate to the playground2. Create a prompt3. Create a dataset4. Add an evaluator5. Run your evaluationNext stepsVideo guideQuickstartsEvaluation quickstartCopy pageCopy pageEvaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don’t always behave predictably — small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.'),\n",
       "  Document(id='0ad6e307-9de5-44c2-a53d-7b06c9565ce7', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b3. Create a dataset\\nClick Set up Evaluation, then use the + New button in the dropdown to create a new dataset.\\nAdd the following examples to the dataset:\\nInputsReference Outputsquestion: Which country is Mount Kilimanjaro located in?output: Mount Kilimanjaro is located in Tanzania.question: What is Earth’s lowest point?output: Earth’s lowest point is The Dead Sea.\\nPress Save to save your newly created dataset.\\n\\u200b4. Add an evaluator\\nClick +Evaluator. Select Correctness from the pre-built evaluator options. Press Save.\\n\\u200b5. Run your evaluation\\nPress Start on the top right to run your evaluation. Running this evaluation will create an experiment that you can view in full by clicking the experiment name.\\n\\n\\u200bNext steps\\nTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.\\n\\n\\nFor more details on evaluations, refer to the Evaluation documentation.\\n\\n\\nLearn how to create and manage datasets in the UI'),\n",
       "  Document(id='2b4477dd-abb0-4ebd-9979-1f5bb8525b07', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='For more details on evaluations, refer to the Evaluation documentation.\\n\\n\\nLearn how to create and manage datasets in the UI\\n\\n\\nLearn how to run an evaluation from the prompt playground\\n\\n\\nIf you prefer video tutorials, check out the Playground videos from the Introduction to LangSmith Course.\\n\\u200bVideo guide\\nWas this page helpful?YesNoTrace an applicationTest promptsAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " 'answer': 'Based on the provided context, one way to evaluate the performance of LLM applications is through the use of evaluations, which are quantitative methods designed to measure how well these models perform. This involves setting up environment variables, creating datasets, defining what you are evaluating, adding evaluators, and running the evaluations to view results. For instance, in the context provided, a dataset is created with specific input-output pairs (like questions and their correct answers), and an evaluator is added to assess correctness. The evaluation is then run to produce an experiment that analyzes the results, helping in identifying any failures and aiding in the development of more reliable AI applications.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7f5c009f-d7ee-47dd-a31f-61ba18708222', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='Evaluations are made up of three components:'),\n",
       " Document(id='de2e94eb-261c-4915-84d5-7e539df4e286', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='up environment variables4. Create a dataset5. Define what you’re evaluating6. Define evaluator7. Run and view resultsNext stepsLangSmith UI1. Navigate to the playground2. Create a prompt3. Create a dataset4. Add an evaluator5. Run your evaluationNext stepsVideo guideQuickstartsEvaluation quickstartCopy pageCopy pageEvaluations are a quantitative way to measure performance of LLM applications, which is important because LLMs don’t always behave predictably — small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.'),\n",
       " Document(id='0ad6e307-9de5-44c2-a53d-7b06c9565ce7', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b3. Create a dataset\\nClick Set up Evaluation, then use the + New button in the dropdown to create a new dataset.\\nAdd the following examples to the dataset:\\nInputsReference Outputsquestion: Which country is Mount Kilimanjaro located in?output: Mount Kilimanjaro is located in Tanzania.question: What is Earth’s lowest point?output: Earth’s lowest point is The Dead Sea.\\nPress Save to save your newly created dataset.\\n\\u200b4. Add an evaluator\\nClick +Evaluator. Select Correctness from the pre-built evaluator options. Press Save.\\n\\u200b5. Run your evaluation\\nPress Start on the top right to run your evaluation. Running this evaluation will create an experiment that you can view in full by clicking the experiment name.\\n\\n\\u200bNext steps\\nTo learn more about running experiments in LangSmith, read the evaluation conceptual guide.\\n\\n\\nFor more details on evaluations, refer to the Evaluation documentation.\\n\\n\\nLearn how to create and manage datasets in the UI'),\n",
       " Document(id='2b4477dd-abb0-4ebd-9979-1f5bb8525b07', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-quickstart', 'title': 'Evaluation quickstart - Docs by LangChain', 'language': 'en'}, page_content='For more details on evaluations, refer to the Evaluation documentation.\\n\\n\\nLearn how to create and manage datasets in the UI\\n\\n\\nLearn how to run an evaluation from the prompt playground\\n\\n\\nIf you prefer video tutorials, check out the Playground videos from the Introduction to LangSmith Course.\\n\\u200bVideo guide\\nWas this page helpful?YesNoTrace an applicationTest promptsAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
